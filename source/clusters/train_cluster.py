import numpy as np
import os

from azureml.core.run import Run
from scipy.stats import entropy

from ..utils.tfrecords import resize, parse_tfrecord
from .kmeans import *
from ..models import *

run = Run.get_context()

class ClusterFeatureMap(tf.keras.Model):
    """"
    This is a clustering class with methods to allow batch clustering
    of the latent representation generated by classifier
    """

    def __init__(self, clustering, classifier, batch_size=16):
        super().__init__()

        self.clustering = clustering
        self.classifier = classifier
        self.batch_size = batch_size

    def train_step(self, data):
        noisy1, label = data[0], data[1]
        _, latent = self.classifier.estimate(noisy1)
        latent = tf.reduce_mean(latent, axis=(1))

        def get_assign():
            return self.clustering.assign(latent)

        def get_initialize():
            return self.clustering.initialize(latent)

        centroid_assignment = tf.cond(self.clustering.initialized, get_assign, lambda: tf.zeros_like(latent[:, 0], dtype=tf.int64))

        def get_update():
            return self.clustering.update(latent, centroid_assignment, label)

        l2_adjustment = self.clustering.compute_distance(latent, centroid_assignment)
        labels_distance = self.clustering.compute_distance_labels(label, centroid_assignment)
        tf.cond(self.clustering.initialized, get_update, get_initialize)

        results = {'cluster_dispersion': tf.reduce_sum(l2_adjustment) / self.batch_size,
                   'cluster_label_distance': tf.reduce_sum(labels_distance) / self.batch_size}

        return results

    def call(self, data):
        noisy1, label = data[0], data[1]
        _, latent = self.classifier(noisy1)
        latent = tf.reduce_mean(latent, axis=(1))

        centroid_assignment = self.cluster.assign(latent)

        return centroid_assignment


class SaveCluster(tf.keras.callbacks.Callback):
    """
    A callback class for saving clusters
    """

    def __init__(self, save_dir):
        super().__init__()
        self.save_dir = save_dir

    def on_epoch_end(self, epoch, logs={}):
        centroids = self.model.clustering.centroids.numpy()
        labels = self.model.clustering.cluster_labels.numpy()

        if hasattr(self.model.clustering, 'centroids_covariance'):
            centroids_covariance = self.model.clustering.centroids_covariance.numpy()
            np.savez(f'{self.save_dir}/centroids.npz', centroids=centroids, centroid_labels=labels, covariance=centroids_covariance)
        else:
            np.savez(f'{self.save_dir}/centroids.npz', centroids=centroids, centroid_labels=labels)

        # -- label entropy per cluster
        labels_without_zeros = labels[labels.sum(-1) > 0]
        prob_labels = labels_without_zeros / labels_without_zeros.sum(-1)[:, None]
        entropy_clusters = entropy(prob_labels, axis=1)
        run.log('entropy_label', entropy_clusters.mean())


class UpdateCluster(tf.keras.callbacks.Callback):
    """
    A callback class for updating centroid coordinates
    """

    def __init__(self):
        super().__init__()

    def on_epoch_end(self, epoch, logs={}):

        tf.cond(self.model.clustering.initialized, self.model.clustering.reset_centroids, lambda: None)
        ch_index = self.model.clustering.compute_calinski_harabasz()
        db_index = self.model.clustering.compute_davies_bouldin()
        db_labels_index = self.model.clustering.compute_davies_bouldin_labels()

        run.log('Calinski-Harabasz Index', float(ch_index))
        run.log('Davies-Bouldin Index', float(db_index))
        run.log('Davies-Bouldin Labels-Based Index', float(db_labels_index))


def get_data_from_tfrecords(args, num_replicas):
    """
    Create a tf.data from tf records in args.train_dir/args.validation_dir
    :param args:
    :param num_replicas:
    :return:
    """

    num_frames = args.num_frames
    num_mel = args.num_mel
    num_labels = args.num_labels

    batch_size = args.batch_size * num_replicas

    autotune = tf.data.AUTOTUNE

    train_filenames = tf.io.gfile.glob(f'{args.train_dir}/*.tfrec')
    train_dataset = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=autotune) \
        .map(lambda example: parse_tfrecord(example,
                                            num_mel=num_mel,
                                            num_frames=num_frames,
                                            snr=args.snr,
                                            labels=args.labels),
             num_parallel_calls=autotune) \
        .map(lambda example: resize(example, num_frames=num_frames,
                                    num_mel=num_mel,
                                    num_labels=args.num_labels,
                                    labels=args.labels,
                                    snr=args.snr),
             num_parallel_calls=autotune) \
        .shuffle(10 * batch_size) \
        .batch(batch_size) \
        .prefetch(autotune) \
        .cache()

    return train_dataset


def get_model(args, num_replicas):
    """
    Construct tensorflow model from checkpoint in args.path_model_tf
    and data loader from args.data_dir
    """

    model = globals()[args.model_name](nclass=args.num_labels)


    if args.path_model_tf is not None:
        model.load_weights(tf.train.latest_checkpoint(args.path_model_tf)).expect_partial()

    cluster_algorithm = globals()[args.clustering_name](args.num_clusters, args.embed_dim)

    clus = ClusterFeatureMap(cluster_algorithm, model, batch_size=args.batch_size * num_replicas)
    clus.compile()
    print('Compiling model done')

    return clus


def train(args):
    """
    Iterate over the batch in the dataset and learn the cluster centers
    using args.clustering_name and args.model_name feature map.
    :param args:
    :return:
    """
    if run._run_id.startswith("OfflineRun"):
        run.number = 0

    strategy = tf.distribute.MirroredStrategy()
    save_dir = args.save_dir
    save_dir = f'{save_dir}/{args.experiment_name}_{run.number}'

    os.makedirs(save_dir, exist_ok=True)

    with strategy.scope():
        model = get_model(args, strategy.num_replicas_in_sync)
        train_loader = get_data_from_tfrecords(args, strategy.num_replicas_in_sync)

    model.fit(train_loader,
              epochs=args.num_epochs,
              callbacks=[SaveCluster(save_dir), UpdateCluster()])
